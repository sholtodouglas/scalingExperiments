{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an exploration in strategies for scaling neural nets to _very large_ models across multiple devices. \n",
    "\n",
    "We'll start off by looking at the basic types of parallelism, then we might explore a more complex strategy which combines elements of the above, such as deepspeed!  \n",
    "\n",
    "- Data parallelism\n",
    "- Model parallelism\n",
    "- Pipeline parallelism\n",
    "- Tensor parallelism \n",
    "\n",
    "A note on hardware: In this notebook we'll use a TPU because the underlying hardware makes it much, much easier (if your needs scale, you can shift to larger and larger TPU pods without issues with inter-machine communication). Later on, we'll look at the classic approach (kubernetes clusters of individual devices) - but I believe in the long term most large model training will occur on mesh networks of devices (like TPUs, or Tesla's dojo). \n",
    "\n",
    "A couple of resources that I've leant on:\n",
    "\n",
    "- [This excellent series on deep learning hardware](https://blog.inten.to/hardware-for-deep-learning-current-state-and-trends-51c01ebbb6dc)\n",
    "- [Lilian Weng's superb notes on training large models](https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)\n",
    "- [Ben Wang's GPT-J - to my knowledge the main published https://github.com/kingoflolz/mesh-transformer-jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.profiler\n",
    "\n",
    "def func1(x):\n",
    "  return jnp.tile(x, 10) * 0.5\n",
    "\n",
    "def func2(x):\n",
    "  y = func1(x)\n",
    "  return y, jnp.tile(x, 10) + 1\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(42), (10000, 1000))\n",
    "y, z = func2(x)\n",
    "\n",
    "z.block_until_ready()\n",
    "\n",
    "jax.profiler.save_device_memory_profile(\"memory.prof\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mMain binary filename not available.\r",
      "\r\n",
      "\u001b[0mType: space\r",
      "\r\n",
      "Entering interactive mode (type \"help\" for commands, \"o\" for options)\r",
      "\r\n",
      "(pprof) "
     ]
    }
   ],
   "source": [
    "!go tool pprof memory.prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36]\n"
     ]
    }
   ],
   "source": [
    "from jax import pmap\n",
    "result = pmap(lambda x: x ** 2)(jnp.arange(7))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5000, 6000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "# create 8 random keys\n",
    "keys = random.split(random.PRNGKey(0), 8)\n",
    "# create a 5000 x 6000 matrix on each device by mapping over keys\n",
    "mats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n",
    "# the stack of matrices is represented logically as a single array\n",
    "mats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a local matmul on each device in parallel (no data transfer)\n",
    "result = pmap(lambda x: jnp.dot(x, x.T))(mats)\n",
    "result.shape\n",
    "result.block_until_ready()\n",
    "jax.profiler.save_device_memory_profile(\"memory.prof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1566595 1.1805978 1.2052746 1.2045677 1.1876795 1.2037715 1.2321935\n",
      " 1.2015157]\n"
     ]
    }
   ],
   "source": [
    "# compute the mean on each device in parallel and print the results\n",
    "print(pmap(jnp.mean)(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
