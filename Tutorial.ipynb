{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal implementation of a multi-device sharded transformer training, and a walk through of each component. The intention is educational - we'll build the required elements from the ground up and understand exactly where each computation is going. This is made exceptionally easy by jax, which is beautiful to work with on hardware meshes. All up it is only ~100 lines more than minGPT, with many of those as comments. Credit where its due - many elements from GPT-J's layer implementations are re-used here, but explained in detail.\n",
    "\n",
    "We'll start off by looking at the basic types of parallelism, then implement a transformer which uses the megatron-LM data+tensor parallelism scheme. In a future, we'll look at pipeline parallelism, implement ZeRO style sharding - and use Ray to coordinate a K8s cluster of TPUv2s (for all those times you don't have a TPUvX-256!)\n",
    "\n",
    "\n",
    "This notebook should be run on a TPU (either through GCP / TRC or Colab) as that gives us 8 devices to experiment with. In general, TPUs make training large models much easier - as your needs scale you can use bigger and bigger TPU pods, so its easy to see why Tesla is making their own extensible hardware mesh in Dojo. \n",
    "\n",
    "A couple of resources that I've leant on:\n",
    "\n",
    "- [Lilian Weng's superb notes on training large models](https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)\n",
    "- [Ben Wang's GPT-J](https://github.com/kingoflolz/mesh-transformer-jax)\n",
    "- [Karpathy's MinGPT](https://github.com/karpathy/minGPT)\n",
    "\n",
    "Note: The memory profiling won't work on colab TPUs as they are running an older version of relevant software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False # set True if on a colab TPU\n",
    "if colab == True:\n",
    "    import jax.tools.colab_tpu\n",
    "    jax.tools.colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.profiler\n",
    "from jax import pmap,value_and_grad\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Let jax de-allocate memory when the object is no longer necessary, slows things down - but keeps our memory profiles more accurate if you\n",
    "# are playing with this notebook rather than progressing straight through. \n",
    "# If you don't set this, memory profiling will be inaccurate when looking at the transformer blocks due to previous allocations. \n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']='platform'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two helper functions which allow us to visualise memory usage\n",
    "\n",
    "def show_call_graph():\n",
    "    !go tool pprof -png memory.prof\n",
    "    img = Image.open('profile001.png')\n",
    "    os.remove('profile001.png')\n",
    "    return img\n",
    "\n",
    "def show_mem(result):\n",
    "    result.block_until_ready()\n",
    "    jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    !go tool pprof -tags memory.prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple data parallel matrix multiplication\n",
    "\n",
    "Lets use pmap to partition initialisation and computation across each TPU node!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_devices = 8\n",
    "\n",
    "batch = 2048\n",
    "embed = 4096\n",
    "w_hidden = 8192\n",
    "\n",
    "keys = random.split(random.PRNGKey(0), x_devices) # [2,x_devices]\n",
    "\n",
    "def matmult(inp,w1):\n",
    "    # run a local matmul on each device in parallel (no data transfer)\n",
    "    result = pmap(jnp.matmul)(inp,w1)\n",
    "    print(result.shape)\n",
    "    show_mem(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the weights matrix sharded across each device with pmap - notice that memory consumption is equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2048, 8192)\n",
      "\u001b[0;31mMain binary filename not available.\r",
      "\r\n",
      "\u001b[0m device: Total 1.8GB\r\n",
      "         224.0MB (12.50%): TPU_0(process=0,(0,0,0,0))\r\n",
      "         224.0MB (12.50%): TPU_1(process=0,(0,0,0,1))\r\n",
      "         224.0MB (12.50%): TPU_2(process=0,(1,0,0,0))\r\n",
      "         224.0MB (12.50%): TPU_3(process=0,(1,0,0,1))\r\n",
      "         224.0MB (12.50%): TPU_4(process=0,(0,1,0,0))\r\n",
      "         224.0MB (12.50%): TPU_5(process=0,(0,1,0,1))\r\n",
      "         224.0MB (12.50%): TPU_6(process=0,(1,1,0,0))\r\n",
      "         224.0MB (12.50%): TPU_7(process=0,(1,1,0,1))\r\n",
      "\r\n",
      " kind: Total 1.8GB\r\n",
      "         1.8GB (  100%): buffer\r\n",
      "       -9.0B (4.8e-07%): executable\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "inp = pmap(lambda key: random.normal(key, (batch, embed), dtype=jnp.float32))(keys) # [x_devices, batch, embed]\n",
    "w1  = pmap(lambda key: random.normal(key, (embed, w_hidden), dtype=jnp.float32))(keys) # [w_hidden, embed]\n",
    "\n",
    "matmult(inp, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an [8,X,X] matrix and distribute it across each device - notice that memory consumption is concentrated on devices[0] because that is where it was implicitly created - despite the fact that the computation is distributed. This underscores the need for both distributed init and compute operations later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2048, 8192)\n",
      "\u001b[0;31mMain binary filename not available.\r",
      "\r\n",
      "\u001b[0m device: Total 2.8GB\r\n",
      "           1.2GB (44.32%): TPU_0(process=0,(0,0,0,0))\r\n",
      "         224.0MB ( 7.95%): TPU_1(process=0,(0,0,0,1))\r\n",
      "         224.0MB ( 7.95%): TPU_2(process=0,(1,0,0,0))\r\n",
      "         224.0MB ( 7.95%): TPU_3(process=0,(1,0,0,1))\r\n",
      "         224.0MB ( 7.95%): TPU_4(process=0,(0,1,0,0))\r\n",
      "         224.0MB ( 7.95%): TPU_5(process=0,(0,1,0,1))\r\n",
      "         224.0MB ( 7.95%): TPU_6(process=0,(1,1,0,0))\r\n",
      "         224.0MB ( 7.95%): TPU_7(process=0,(1,1,0,1))\r\n",
      "\r\n",
      " kind: Total 2.8GB\r\n",
      "          2.8GB (  100%): buffer\r\n",
      "       -11.0B (3.7e-07%): executable\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "inp = pmap(lambda key: random.normal(key, (batch, embed), dtype=jnp.float32))(keys) # [x_devices, batch, embed]\n",
    "w1_dist  = random.normal(random.PRNGKey(0), (x_devices, embed, w_hidden), dtype=jnp.float32) # [w_hidden, embed]\n",
    "\n",
    "\n",
    "matmult(inp, w1_dist)\n",
    "\n",
    "del w1_dist # gc this so that it doesn't clog up memory if the cell is re-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor parallelism\n",
    "\n",
    "What if we have a layer or model that is too big to fit on a single device? \n",
    "\n",
    "The great thing about TPUs is that they are a hardware _mesh_ - and when we use xmap we can use named axis to re-arrange these at will to achieve both data and model parallelism.\n",
    "\n",
    "How does model parallelism work? For starters, lets look at [Megatron LM's method](https://arxiv.org/pdf/1909.08053.pdf), which is what was used for GPT-J. It has a great balance of simplicity and effectiveness, and is very efficient provided you have a hardware mesh. Luckily we do!\n",
    "\n",
    "- Megatron LM: Takes advantage of the fact that  matrix multiplication and self-attention can be calculated across separate devices with minimal communication. Megatron's approach optimises for minimising communication overheads at the expense of some increase in memory. E.g., the layer norm parameters are duplicated, and take the output of the previous layer. \"Since all parameters are either local or duplicated, there is no need for communicating updated parameters.\"\n",
    "\n",
    "How does it work in the case of a simple matrix multiplication + nonlinearity?\n",
    "\n",
    "$$ Y  = GELU(XA) $$ \n",
    "\n",
    "If we split A along it's columns A = [A_1, A_2], we can perform the calculation and apply the non-linearity indepedently.\n",
    "\n",
    "$$ Y  = [Y_1, Y_2] = [GeLU(XA_1), GeLU(XA_2)]$$\n",
    "\n",
    "In the next section we'll look at how it works with multiple MMs and self-attention, first lets introduce x-map and distribute a computation with it. Note - <b> as in the second example above because we are not distributing the creation of the inputs and weights matrices the memory will be concentrated on the first device <b>. In the next section we'll look at how to use xmap to also distribute the initialisation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.maps import mesh, xmap\n",
    "import numpy as np\n",
    "batch = 2048\n",
    "embed = 4096\n",
    "w_hidden = 16384\n",
    "\n",
    "inp = random.normal(random.PRNGKey(0), (batch, embed), dtype=jnp.float32) # [batch, embed]\n",
    "w1  = random.normal(random.PRNGKey(0), (w_hidden, embed), dtype=jnp.float32) # [w_hidden, embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape our 8 devices into a 4,2 mesh with axis names x,y\n",
    "x_devices = 4\n",
    "y_devices = 8//x_devices\n",
    "\n",
    "axis_names = ('x', 'y')\n",
    "mesh_devices = np.array(jax.devices()).reshape((x_devices, y_devices))\n",
    "mesh_def = (mesh_devices, axis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/jax/experimental/maps.py:418: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mMain binary filename not available.\n",
      "\u001b[0m device: Total 416.0MB\n",
      "         304.0MB (73.08%): TPU_0(process=0,(0,0,0,0))\n",
      "          16.0MB ( 3.85%): TPU_1(process=0,(0,0,0,1))\n",
      "          16.0MB ( 3.85%): TPU_2(process=0,(1,0,0,0))\n",
      "          16.0MB ( 3.85%): TPU_3(process=0,(1,0,0,1))\n",
      "          16.0MB ( 3.85%): TPU_4(process=0,(0,1,0,0))\n",
      "          16.0MB ( 3.85%): TPU_5(process=0,(0,1,0,1))\n",
      "          16.0MB ( 3.85%): TPU_6(process=0,(1,1,0,0))\n",
      "          16.0MB ( 3.85%): TPU_7(process=0,(1,1,0,1))\n",
      "\n",
      " kind: Total 416.0MB\n",
      "        416.0MB (  100%): buffer\n",
      "       -16.0B (3.7e-06%): executable\n",
      "\n",
      "(2048, 16384)\n"
     ]
    }
   ],
   "source": [
    "def matmult(x, w1):\n",
    "    '''\n",
    "    x: [x_devices: the number of devices we are partitioning across\n",
    "        batch: ..\n",
    "        embed: .. ]\n",
    "    w1: [w_hidden: will be partioned evenly over the columns of the mesh \n",
    "         embed: the hidden size, but it is partitioned ... ]\n",
    "    '''\n",
    "\n",
    "    res = jnp.matmul(x,w1)\n",
    "    mean_embed = jnp.sum(res, axis=['batch']) / 8096\n",
    "    return mean_embed\n",
    "\n",
    "with mesh(*mesh_def):\n",
    "    out = xmap(matmult, \n",
    "              in_axes = (['batch',  ...], ['w_hidden', ...]),\n",
    "              out_axes = ['batch', 'w_hidden', ...],\n",
    "              axis_resources={'batch': 'x', 'w_hidden': 'y'})(inp, w1)\n",
    "    \n",
    "    \n",
    "    show_mem(out)\n",
    "    print(out.shape)\n",
    "    \n",
    "del out, w1, inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really cool! With x-map, we can just take the same size inputs [2048, 4096] and [4096, 16384], and perform the computation in both a data and batch parallel fashion. The batch axis is split across 4 machines, the weight across 2 - and it comes back whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A transformer block\n",
    "\n",
    "- Ben Wang's GPT-J has an excellent, production ready example. Here, we'll draw from the design choices of both minGPT and GPT-J in an effort to make a minimal, but complete example. \n",
    "- The key fact that we're taking advantage of is that sections of the embedding are allocated to differnet self-attention 'heads' - which means that we can split across as long as the the # heads is divisible by the number of shards. E.g. if our embedding is 2048, and we have 8 heads of 256 each - we could split the embedding across 2 shards, alllocating 4 heads per shard. See the following diagram from the megatron-LM paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "\n",
    "\n",
    "GPT1Config = {\n",
    "    'n_vocab': 5000,\n",
    "    'block_size': 32,\n",
    "    'n_layer' : 12,\n",
    "    'n_head' : 12,\n",
    "    'd_model' : 768,\n",
    "    'shards': 2}\n",
    "\n",
    "\n",
    "class TransformerLayerShard(hk.Module):\n",
    "        '''\n",
    "        # A simple transformer layer shard that exists on one of the devices. \n",
    "        '''\n",
    "        def __init__(self, config, name=None, init_scale=1.):\n",
    "            super().__init__(name=name)\n",
    "            heads = config[\"n_head\"]\n",
    "            dim = config[\"d_model\"]\n",
    "            self.shards  = config[\"shards\"]\n",
    "            \n",
    "            assert dim % heads == 0 \n",
    "            assert heads % self.shards == 0 \n",
    "            \n",
    "            self.dim_per_head = dim // heads\n",
    "            self.dim_per_shard = dim // self.shards\n",
    "            self.heads_per_shard = heads // self.shards\n",
    "            \n",
    "            # GPT-J uses a common layer norm between the mlp and the self attention, minGPT uses different layers - lets go with one for now. Much of a muchness.\n",
    "            self.ln = hk.LayerNorm(-1, True, True)\n",
    "            \n",
    "            # key, query and value projections for all heads on this shard\n",
    "            self.q = hk.Linear(self.dim_per_shard, with_bias=False)\n",
    "            self.k = hk.Linear(self.dim_per_shard, with_bias=False)\n",
    "            self.v = hk.Linear(self.dim_per_shard, with_bias=False)\n",
    "            \n",
    "            # self att output projection\n",
    "            self.att_proj = hk.Linear(dim, with_bias=False, w_init=hk.initializers.TruncatedNormal(stddev=init_scale / np.sqrt(dim)))\n",
    "            \n",
    "            # feedforward layers\n",
    "            self.dense_proj = hk.Linear(self.dim_per_shard * 4)\n",
    "            self.dense_proj_out = hk.Linear(dim,\n",
    "                                      w_init=hk.initializers.TruncatedNormal(stddev=init_scale / np.sqrt(dim)))\n",
    "            \n",
    "            \n",
    "        def self_attention(self, q, k ,v, bias):\n",
    "            '''\n",
    "            k,q,v: [T, heads_per_shard, dim_per_head]\n",
    "            '''\n",
    "            T, _, _ = k.shape\n",
    "            \n",
    "            # No batch dimension needed in the einsum because it is abstracted away by the xmap \n",
    "            attention_logits = jnp.einsum('thd,Thd->htT', q, k) # [heads_per_shard, T,T]\n",
    "            sqrt_key_size = np.sqrt(self.dim_per_head).astype(k.dtype) # [1,]\n",
    "            \n",
    "            attention_logits = attention_logits/sqrt_key_size # [heads_per_shard, T,T]\n",
    "            \n",
    "            attention_logits += bias # [B, heads_per_shard, T,T]\n",
    "            \n",
    "            attention_weights = jax.nn.softmax(attention_logits)  # [heads_per_shard, T,T]\n",
    "            \n",
    "            weighted_values = jnp.einsum('htT, Thd->thd', attention_weights, v).reshape((T, self.dim_per_shard)) # [T, dim_per_shard]\n",
    "            \n",
    "            return self.att_proj(weighted_values)\n",
    "        \n",
    "        def feed_forward(self, x):\n",
    "            '''\n",
    "            x: [T,embed_dim]\n",
    "            '''\n",
    "            dense_proj = self.dense_proj(x)\n",
    "            dense_proj = jax.nn.gelu(dense_proj)\n",
    "            return self.dense_proj_out(dense_proj)\n",
    "            \n",
    "            \n",
    "        def qkv_proj(self, x): \n",
    "            '''\n",
    "            x: [T, embed_dim]\n",
    "            '''\n",
    "            q = self.q(x).reshape(x.shape[:-1] + (self.heads_per_shard, self.dim_per_head)) # [T, heads_per_shard, dim_per_head]\n",
    "            v = self.v(x).reshape(x.shape[:-1] + (self.heads_per_shard, self.dim_per_head)) # \"\"\n",
    "            k = self.k(x).reshape(x.shape[:-1] + (self.heads_per_shard, self.dim_per_head)) # \"\" \n",
    "            \n",
    "            return q,k,v\n",
    "        \n",
    "        \n",
    "        def __call__(self, x):\n",
    "            '''\n",
    "            x: [T, embed_dim]\n",
    "            '''\n",
    "            # preliminaries    \n",
    "#             print(x.shape)\n",
    "            T,C = x.shape\n",
    "            x = self.ln(x) # [T,embed_dim]\n",
    "            \n",
    "            # causal self attention\n",
    "            q,k,v = self.qkv_proj(x)\n",
    "            causal_mask = np.tril(np.ones((T,T))) # [T,T]\n",
    "            bias = -1e10 * (1. - causal_mask) # [T,T]\n",
    "            attn = self.self_attention(q, k ,v, bias)  # [T,embed_dim]\n",
    "            \n",
    "            # feedforward\n",
    "            ff  = self.feed_forward(x) # [B,T,embed_dim]\n",
    "            \n",
    "            # block\n",
    "            x = x + attn # [T,embed_dim]\n",
    "            x = x + ff # [T,embed_dim]\n",
    "            \n",
    "            # We finally need to sum across shards to collect the information from each head into the new embedding. \n",
    "            # The idea of heads 'adding to the information stream' is a nice way of thinking about transformers from Anthropic's \n",
    "            # latest work. \n",
    "            # In the full GPT-J implementation, they've defined a custom operator which does the psum on the forward pass but\n",
    "            # is the identity function on the backward pass - currently testing how necessary that is.\n",
    "            return jax.lax.psum(x, \"shard\")\n",
    "\n",
    "        \n",
    "                \n",
    "def model_fn(x):\n",
    "    model = TransformerLayerShard(GPT1Config)\n",
    "    return model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would be possible to test the above block without xmap if we got rid of the psum - but I'm going to be slightly opinionated here despite the boilerplate setup.\n",
    "batch = 4\n",
    "shard = 2\n",
    "\n",
    "axis_names = ('dp', 'mp')\n",
    "mesh_devices = np.array(jax.devices()).reshape((batch, shard))\n",
    "mesh_def = (mesh_devices, axis_names)\n",
    "\n",
    "\n",
    "init = jax.experimental.maps.xmap(fun=hk.transform(model_fn).init,\n",
    "                                  in_axes=([\"shard\", ...],\n",
    "                                           [\"batch\", ...]),\n",
    "                                  out_axes=[\"shard\", ...],\n",
    "                                  axis_resources={'shard': 'mp', 'batch': 'dp'})\n",
    "\n",
    "forward = jax.experimental.maps.xmap(fun=hk.without_apply_rng(hk.transform(model_fn)).apply,\n",
    "                                     in_axes=([\"shard\", ...],\n",
    "                                              [\"batch\", ...]),\n",
    "                                     out_axes=[\"batch\", ...],\n",
    "                                     axis_resources={'shard': 'mp', 'batch': 'dp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mMain binary filename not available.\n",
      "\u001b[0m device: Total 112.7MB\n",
      "         15.4MB (13.66%): TPU_0(process=0,(0,0,0,0))\n",
      "         13.9MB (12.33%): TPU_1(process=0,(0,0,0,1))\n",
      "         13.9MB (12.33%): TPU_2(process=0,(1,0,0,0))\n",
      "         13.9MB (12.33%): TPU_3(process=0,(1,0,0,1))\n",
      "         13.9MB (12.33%): TPU_4(process=0,(0,1,0,0))\n",
      "         13.9MB (12.33%): TPU_5(process=0,(0,1,0,1))\n",
      "         13.9MB (12.33%): TPU_6(process=0,(1,1,0,0))\n",
      "         13.9MB (12.33%): TPU_7(process=0,(1,1,0,1))\n",
      "\n",
      " kind: Total 112.7MB\n",
      "        112.7MB (  100%): buffer\n",
      "       -23.0B (1.9e-05%): executable\n",
      "\n",
      "(16, 32, 768)\n"
     ]
    }
   ],
   "source": [
    "key = hk.PRNGSequence(42)\n",
    "x = jax.random.uniform(next(key), (16, 32, 768), minval=0, maxval=255).astype(jnp.float32)  # [B,T,embed]\n",
    "\n",
    "with jax.experimental.maps.mesh(*mesh_def):\n",
    "    state = init(jnp.array(key.take(shard)), x)\n",
    "\n",
    "    o = forward(state, x)\n",
    "    show_mem(o)\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a transformer\n",
    "\n",
    "Now all we're missing is an embedding layer to transform a sequence of discrete tokens to a dense vector - and an output projection layer to send it back! \n",
    "\n",
    "\n",
    "## Embedding layer\n",
    "Functionally, this is a sequence of [Batch, Time, Vocab_size] multiplied by a [Vocab_size, embedding_dim] matrix to project the one hot sequence into a dense embedding. \n",
    "\n",
    "There are two approaches that could make sense:\n",
    "\n",
    "1. Partition the embedding matrix along the vocabulary dimension. The implementation of this in GPT-J is cheeky:\n",
    "    i. Assign a subset of token indices to each shard. E.g. if our vocab dimension is 50k, 0-25k correspond to shard 0, 25k-50k correspond to shard 1.\n",
    "    ii. In the one-hot expansion of the input sequence, zero anything which lies outside this range. This means that the projection matrix on each shard only needs to be [25k,embedding_dim], but the zero-d tokens will not contribute to the embedding on that shard.\n",
    "    iii. psum across the shards to get the full embedding. \n",
    "    \n",
    "    \n",
    "2. Alternatively, partition along the embedding dimension. In this case, the embedding matrix on each shard would be [50k, embedding_dim//shards], every token would be fully expanded into the one-hot representation and an all_gather would be required across the shards to concatenate the embedding dim of the outputs.\n",
    "\n",
    "In both cases  <b> the embedding weights matrix will be the same size  </b> (it doesn't matter which dimension is divided by shards),  <b> but the methods differ in terms of the space allocated for the input sequence</b>. The second method requires the full input sequence be expanded to the full one-hot representation on each device, but the first method divides the size of that representation by the number of shards - saving more space. If you're token space is in characters, this doesn't matter - but some token spaces have 10s of thousands of entries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingShard(hk.Module):\n",
    "    def __init__(self, config, name=None):\n",
    "        super().__init__(name=name)\n",
    "        in_dim = config[\"n_vocab\"]\n",
    "        out_dim = config[\"d_model\"]\n",
    "        shards = config[\"shards\"]\n",
    "\n",
    "        assert in_dim % shards == 0\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim_per_shard = in_dim // shards\n",
    "        self.out_dim_per_shard = out_dim // shards\n",
    "\n",
    "        embed_init = hk.initializers.TruncatedNormal(stddev=0.02)\n",
    "        self.positional_embeddings = hk.get_parameter('pos_embs', [config[\"block_size\"], self.out_dim_per_shard], init=embed_init)\n",
    "\n",
    "        # notice unlike the ff transformer layer, this linear layer has the full output dimension because we are partitioning across vocab. \n",
    "        self.proj = hk.Linear(self.out_dim, w_init=hk.initializers.TruncatedNormal(stddev=1 / np.sqrt(in_dim)))\n",
    "\n",
    "    def __call__(self, x, dtype=jnp.bfloat16):\n",
    "        \n",
    "        # work out which shard we are on, and the start token index\n",
    "        shard_start_index = jax.lax.axis_index('shard') * self.in_dim_per_shard\n",
    "        \n",
    "        # subtract the shard_start_index from the input indices. This means anything below it will be zero-d (as it will be a negative number)\n",
    "        # at the same time, anything above 'in_dim_per_shard' will also be zero-d. This means that each shard gets a window of in_dim_per_shard indices\n",
    "        # which it will expand to a one-hot representation - saving lots of space!\n",
    "        input_onehot = jax.nn.one_hot(x - shard_start_index, self.in_dim_per_shard)\n",
    "        proj_out = self.proj(input_onehot)\n",
    "        # sum across shards to create a full embedding\n",
    "        proj_out = jax.lax.psum(proj_out, \"shard\")\n",
    "        # gets all of the positional embeddings as split across each shard (column wise split of positional embeddings)\n",
    "        all_pos_embed = jax.lax.all_gather(self.positional_embeddings, 'shard')\n",
    "        # flattens them, so now there are identical, complete positional embeddings on each device\n",
    "        all_pos_embed = hk.Flatten()(jnp.transpose(all_pos_embed, (1, 0, 2)))\n",
    "\n",
    "        proj_out += all_pos_embed[:proj_out.shape[0]] # only do the embeddings up to the length of the input sequence, to allow for variable input size\n",
    "\n",
    "        return proj_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection layer\n",
    "\n",
    "The projection layer is much more similar to the simple matrix multiplication example above. As we'll have the full embedding as an output of transformer layer below we partition across the output dim, then all gather to get the final output embedding in vocab space. \n",
    "\n",
    "Where it gets trickier is how this interacts with the loss calculation. The [batch, time, vocab_size] matrix output above is large, and we'd prefer to only generate it when necesary (e.g. during inference). During training, could we calculate the per shard loss components and then only cross-communicate the smaller [batch, time] cross entropy loss values?\n",
    "\n",
    "Yes! We'll still need some cross communication (e.g. to calculate the max of the logits to stabilise it numerically, or calculate the denominator of the softmax equation), but this comes with no memory penalty!\n",
    "\n",
    "This is directly lifted from GPT-J, but I'll comment extensively so it is clear what is going on.\n",
    "\n",
    "The loss function isn't the typical implementation of softmax cross entropy, so lets simplify it down to explain it. The implementation is convenient and stable - minimising communication overheads across shards to a limited set of sums. If the loss function is confusing - jump to the bottom for a simplified implementation with some test inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class ProjectionShard(hk.Module):\n",
    "    def __init__(self, config, name=None):\n",
    "        super().__init__(name=name)\n",
    "        out_dim = config[\"n_vocab\"]\n",
    "        shards = config[\"shards\"]\n",
    "\n",
    "        assert out_dim % shards == 0\n",
    "\n",
    "        self.dim = out_dim\n",
    "        self.dim_per_shard = out_dim // shards\n",
    "\n",
    "        self.norm = hk.LayerNorm(-1, True, True)\n",
    "\n",
    "        self.proj = hk.Linear(self.dim_per_shard)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.norm(x)\n",
    "        proj = self.proj(x)\n",
    "\n",
    "        all_proj = jax.lax.all_gather(proj, 'shard')\n",
    "\n",
    "        return hk.Flatten()(jnp.transpose(all_proj, (1, 0, 2)))\n",
    "\n",
    "    def loss(self, x, targets, z_loss=1):\n",
    "        \n",
    "        '''\n",
    "        x: [T, dim_per_shard]\n",
    "        targets: [T]\n",
    "        '''\n",
    "        x = self.norm(x)\n",
    "        # calculate logits on a per shard basis\n",
    "        logits = self.proj(x) # [T, dim_per_shard]\n",
    "        # get the max of logits per dimension across the shards. Use this to prevent both under and overflow by subtracting it from every logit.\n",
    "        # For an explaination on why you need this - see the opening pages of chapter 4 'Numerical computation' in goodfellow's deep learning book. \n",
    "        global_max = jax.lax.pmax(jax.lax.stop_gradient(logits.max(-1, keepdims=True)), \"shard\")\n",
    "        logits -= jax.lax.stop_gradient(global_max) # [T, dim_per_shard]\n",
    "        \n",
    "        # As we are computing the output vocab matrix in a sharded fashion, only get the targets corresponding to that shard\n",
    "        # using the same trick as used in the embedding matrix.\n",
    "        shard_start_index = jax.lax.axis_index('shard') * self.dim_per_shard\n",
    "        gt_onehot = jax.nn.one_hot(targets - shard_start_index, self.dim_per_shard) # [T, dim_per_shard]\n",
    "        # this is a point multiplication, so it zeros out anything which isn't a 1 in the one-hot representation. \n",
    "        # then sums along the embedding axis. See above code snippet for explaination for the next few lines.\n",
    "        predicted_logits = jnp.sum(jnp.multiply(gt_onehot, logits), axis=-1) # [T]\n",
    "        # subtract the summed logit from the summed 'predicted_logit'\n",
    "        # Any entry but the correct one is 0 in 'predicted logit' - and due to the max used for stabilisation\n",
    "        # the entry of the highest index will be 0. Therefore, the subtraction of the two will draw the highest index to the correct one.\n",
    "        # By only working with sums when we are using the sharded version we minimise communication.\n",
    "        predicted_logits = jax.lax.psum(predicted_logits, 'shard') \n",
    "        exp_logits = jnp.exp(logits)\n",
    "        sum_exp_logits = exp_logits.sum(axis=-1)\n",
    "        sum_exp_logits = jax.lax.psum(sum_exp_logits, 'shard')\n",
    "        loss = jnp.log(sum_exp_logits) - predicted_logits\n",
    "        # An additional loss which keeps the logits small - avoiding roundoff errors in bfloat16 (according to the mesh tensorflow repo).\n",
    "        loss += (1e-4 * jnp.square(jnp.log(sum_exp_logits)) * z_loss).mean()\n",
    "\n",
    "        # Due to the above, it is easy to correctly predict accuracy. \n",
    "        correct = (0.0 == predicted_logits)\n",
    "        return loss.sum(), correct\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharded transformer\n",
    "\n",
    "This is easy - just combine the sharded layers as one would a transformer on one device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTransformerShard(hk.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        heads = config[\"n_head\"]\n",
    "        shards = config[\"shards\"]\n",
    "        layer_count = config[\"n_layer\"]\n",
    "\n",
    "        self.layers = []\n",
    "        self.heads = heads\n",
    "\n",
    "        self.heads_per_shard = heads // shards\n",
    "\n",
    "        self.embed = EmbeddingShard(config)\n",
    "\n",
    "        init_scale = 2. / layer_count\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            self.layers.append(TransformerLayerShard(config, name=f\"layer_{i}\", init_scale=init_scale))\n",
    "\n",
    "        self.proj = ProjectionShard(config)\n",
    "        \n",
    "        \n",
    "    def trunk(self, tokens):\n",
    "        x = self.embed(tokens)\n",
    "\n",
    "        for l in self.layers:\n",
    "            x = x + l(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, tokens, targets):\n",
    "        x = self.trunk(tokens)\n",
    "        l, acc = self.proj.loss(x, targets)\n",
    "        return l\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        \n",
    "        x = self.trunk(tokens)\n",
    "        return self.proj(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "The below is mostly a mix of haiku, optax and xmap boilerplate.\n",
    "\n",
    "The new elements are:\n",
    "- A dataloader: Simple, character based shakespeare. \n",
    "- An optimiser: Uses optax - carefully note where the init and update functions are (within xmap - to ensure the memory stays allocated to the correct device).\n",
    "- Save/restore functions: An extremely unoptimised save/restore function. Better implementations would parallel process it, but that isn't necessary yet at TPUv2 model sizes.\n",
    "\n",
    "And give it a crack! Bump up the layer size, then set shards = 1 and devices = 8 (to make it purely data parallel). You'll notice you hit memory errors that you wouldn't hit with an equivalent model size with two shards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import tensorflow as tf\n",
    "\n",
    "GPTConfig = {\n",
    "    'n_vocab': 66,\n",
    "    'block_size': 32,\n",
    "    'n_layer' : 3,\n",
    "    'n_head' : 8,\n",
    "    'd_model' : 768,\n",
    "    'shards': 2,\n",
    "    'devices': 4,\n",
    "    'batch_size_per_parallel': 256,\n",
    "    'ckpt_dir': 'test'}\n",
    "\n",
    "\n",
    "# A downside of using the more memory efficient method of embedding sharding is that it requires equal shard size across devices\n",
    "# or a 'check which device I'm on, lookup desired shard size'. For the moment - easier to just have equal shard size and a few empty spots for tokens.\n",
    "\n",
    "assert GPTConfig['n_vocab'] % GPTConfig['shards'] == 0\n",
    "\n",
    "class Dataloader():\n",
    "    '''\n",
    "    A super simple dataloader for a tiny dataset. Better implementations would \n",
    "    - Pre-process the data into tf-records, and stream this from \n",
    "        a GCP bucket. \n",
    "    - Eliminate unncessary copies (e.g. as_numpy_iterator)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not os.path.exists('input.txt'):\n",
    "            os.system('wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "            \n",
    "        text = open('input.txt', 'r').read() \n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.vocab_len = len(self.vocab)\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.vocab) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.vocab) }\n",
    "        tokens = [self.stoi[c] for c in text]\n",
    "        d = tf.data.Dataset.from_tensor_slices(tokens)\n",
    "        d = d.batch(config['block_size']+1, drop_remainder=True) # +1 because [:-1] will be x, and [1:] will be y\n",
    "        self.d = iter(d.batch(config['batch_size_per_parallel']*config['devices'], drop_remainder=True).repeat().as_numpy_iterator())\n",
    "        \n",
    "    def next_batch(self):\n",
    "        b = self.d.next()\n",
    "        return b[:, :-1], b[:, 1:] # x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTransformer():\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        axis_names = ('batch', 'shard')\n",
    "        mesh_devices = np.array(jax.devices()).reshape((config['devices'], config['shards']))\n",
    "        self.mesh_def = (mesh_devices, axis_names)\n",
    "\n",
    "        self.config = config\n",
    "        self.optimizer = optax.adam(1e-5)\n",
    "        \n",
    "        self.key = hk.PRNGSequence(42)\n",
    "        \n",
    "        self.init = jax.experimental.maps.xmap(fun=self.init_state,\n",
    "                                  in_axes=([\"shard\", ...], # rngs\n",
    "                                           [\"batch\", ...]), # x\n",
    "                                  out_axes=[\"shard\", ...],\n",
    "                                  axis_resources={'shard': 'shard', 'batch': 'batch'})\n",
    "\n",
    "        self.forward = jax.experimental.maps.xmap(fun=self.eval_step,\n",
    "                                             in_axes=([\"shard\", ...], # params\n",
    "                                                      [\"batch\", ...]), # x \n",
    "                                             out_axes=([\"batch\", ...]), \n",
    "                                             axis_resources={'shard': 'shard', 'batch': 'batch'})\n",
    "\n",
    "\n",
    "        self.train = jax.experimental.maps.xmap(fun=self.train_step,\n",
    "                                             in_axes=([\"shard\", ...], # state\n",
    "                                                      [\"batch\", ...], # x\n",
    "                                                      [\"batch\", ...]),# y\n",
    "                                             out_axes=([['batch'],           # loss\n",
    "                                                       ['shard',...]]), # state\n",
    "                                             axis_resources={'shard': 'shard', 'batch': 'batch'})\n",
    "\n",
    "\n",
    "    # Haiku pure functions for convenience \n",
    "    def eval_fn(self, x):\n",
    "        model = CausalTransformerShard(self.config)\n",
    "        return model(x)\n",
    "\n",
    "    def train_fn(self, x,y):\n",
    "        model = CausalTransformerShard(self.config)\n",
    "        return model.loss(x,y)\n",
    "\n",
    "\n",
    "    def init_state(self, key, x):\n",
    "        '''\n",
    "        A parallelised init function that ensures optimiser params are stored on the respective devices. \n",
    "        '''\n",
    "        params = hk.transform(self.eval_fn).init(key, x)\n",
    "\n",
    "        return {\n",
    "            \"params\": params,\n",
    "            \"step\": np.array(0),\n",
    "            \"opt_state\": self.optimizer.init(params)\n",
    "        }\n",
    "\n",
    "    def eval_step(self, params, x):\n",
    "\n",
    "        forward_fn = hk.without_apply_rng(hk.transform(self.eval_fn))\n",
    "        out = forward_fn.apply(params, x)\n",
    "        return out\n",
    "\n",
    "    def train_step(self, state, x,y):\n",
    "\n",
    "        l_fn = hk.without_apply_rng(hk.transform(self.train_fn))\n",
    "        loss, grads = value_and_grad(l_fn.apply)(state['params'], x,y)\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        updates, new_opt_state = self.optimizer.update(grads, state['opt_state'], state['params'])\n",
    "\n",
    "        return loss, {\n",
    "            \"params\": optax.apply_updates(state['params'], updates),\n",
    "            \"step\": state['step'] + 1,\n",
    "            \"opt_state\": new_opt_state\n",
    "        }\n",
    "    \n",
    "    def save(self, state):\n",
    "        os.makedirs(self.config['ckpt_dir'], exist_ok=True)\n",
    "        with open(os.path.join(self.config['ckpt_dir'], \"arrays.npy\"), \"wb\") as f:\n",
    "            for x in jax.tree_leaves(state):\n",
    "                np.save(f, x, allow_pickle=False)\n",
    "\n",
    "        tree_struct = jax.tree_map(lambda t: 0, state)\n",
    "        with open(os.path.join(self.config['ckpt_dir'], \"tree.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(tree_struct, f)\n",
    "\n",
    "    def restore(self):\n",
    "        '''\n",
    "        Usage: set state = model.restore() after initialising the model. \n",
    "        '''\n",
    "        with open(os.path.join(self.config['ckpt_dir'], \"tree.pkl\"), \"rb\") as f:\n",
    "            tree_struct = pickle.load(f)\n",
    "\n",
    "        leaves, treedef = jax.tree_flatten(tree_struct)\n",
    "        with open(os.path.join(self.config['ckpt_dir'], \"arrays.npy\"), \"rb\") as f:\n",
    "            flat_state = [np.load(f) for _ in leaves]\n",
    "\n",
    "        return jax.tree_unflatten(treedef, flat_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mMain binary filename not available.\r",
      "\r\n",
      "\u001b[0m device: Total 997.8MB\r\n",
      "         124.7MB (12.50%): TPU_0(process=0,(0,0,0,0))\r\n",
      "         124.7MB (12.50%): TPU_1(process=0,(0,0,0,1))\r\n",
      "         124.7MB (12.50%): TPU_2(process=0,(1,0,0,0))\r\n",
      "         124.7MB (12.50%): TPU_3(process=0,(1,0,0,1))\r\n",
      "         124.7MB (12.50%): TPU_4(process=0,(0,1,0,0))\r\n",
      "         124.7MB (12.50%): TPU_5(process=0,(0,1,0,1))\r\n",
      "         124.7MB (12.50%): TPU_6(process=0,(1,1,0,0))\r\n",
      "         124.7MB (12.50%): TPU_7(process=0,(1,1,0,1))\r\n",
      "\r\n",
      " kind: Total 997.8MB\r\n",
      "        997.8MB (  100%): buffer\r\n",
      "       -25.0B (2.4e-06%): executable\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds = Dataloader(GPTConfig)\n",
    "model = CausalTransformer(GPTConfig)\n",
    "\n",
    "with jax.experimental.maps.mesh(*model.mesh_def):\n",
    "    x,y = ds.next_batch() # [B,T], [B,T]\n",
    "    state = model.init(jnp.array(model.key.take(GPTConfig['shards'])), x)\n",
    "    o = model.forward(state['params'], x)\n",
    "    show_mem(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_device_pixel_ratio', {\n",
       "                device_pixel_ratio: fig.ratio,\n",
       "            });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'dblclick',\n",
       "        on_mouse_event_closure('dblclick')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    fig.rubberband_canvas.style.cursor = msg['cursor'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            var img = evt.data;\n",
       "            if (img.type !== 'image/png') {\n",
       "                /* FIXME: We get \"Resource interpreted as Image but\n",
       "                 * transferred with MIME type text/plain:\" errors on\n",
       "                 * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "                 * to be part of the websocket stream */\n",
       "                img.type = 'image/png';\n",
       "            }\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                img\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from https://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * https://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.key === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.key;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.key !== 'Control') {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    else if (event.altKey && event.key !== 'Alt') {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    else if (event.shiftKey && event.key !== 'Shift') {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k' + event.key;\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.binaryType = comm.kernel.ws.binaryType;\n",
       "    ws.readyState = comm.kernel.ws.readyState;\n",
       "    function updateReadyState(_event) {\n",
       "        if (comm.kernel.ws) {\n",
       "            ws.readyState = comm.kernel.ws.readyState;\n",
       "        } else {\n",
       "            ws.readyState = 3; // Closed state.\n",
       "        }\n",
       "    }\n",
       "    comm.kernel.ws.addEventListener('open', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('close', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('error', updateReadyState);\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        var data = msg['content']['data'];\n",
       "        if (data['blob'] !== undefined) {\n",
       "            data = {\n",
       "                data: new Blob(msg['buffers'], { type: data['blob'] }),\n",
       "            };\n",
       "        }\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(data);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640.0000169542105\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 37.25770568847656:  41%|â–ˆâ–ˆâ–   | 40878/100000 [50:54<1:13:37, 13.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116757/2187369808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss.mean()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/experimental/maps.py\u001b[0m in \u001b[0;36mfun_mapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    522\u001b[0m                            \u001b[0;34mf\"which asserts that it should be of rank {spec.expected_rank}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                            f\"but the argument has rank {arg.ndim} (and shape {arg.shape})\")\n\u001b[0;32m--> 524\u001b[0;31m     out_flat = xmap_p.bind(\n\u001b[0m\u001b[1;32m    525\u001b[0m       \u001b[0mfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<unnamed function>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/experimental/maps.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_axes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1554\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/experimental/maps.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_xmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/experimental/maps.py\u001b[0m in \u001b[0;36mxmap_impl\u001b[0;34m(fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes, axis_resources, resource_env, backend, spmd_in_axes, spmd_out_axes_thunk, *args)\u001b[0m\n\u001b[1;32m    557\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0;34m\"mesh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_mesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                         (\"abstract args\", in_avals))\n\u001b[0;32m--> 559\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mxmap_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mexecute_replicated\u001b[0;34m(compiled, backend, in_handler, out_handler, *args)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_replicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded_on_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbufs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "losses = []\n",
    "with jax.experimental.maps.mesh(*model.mesh_def):\n",
    "    steps = [t for t in range(0, 100000)]\n",
    "    pbar = tqdm(steps)\n",
    "    for t in pbar:\n",
    "        x,y = ds.next_batch()\n",
    "        loss, state = model.train(state, x,y)\n",
    "        if t % 100 == 0:\n",
    "            pbar.set_description(f\"Loss: {loss.mean()}\")\n",
    "            losses.append(loss.mean())\n",
    "            ax.clear()\n",
    "            ax.plot(steps[:len(losses)], losses)\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ed sonrt?\n",
      "wor theu ahalt taar h\n",
      "--------------------------\n",
      "et I  Tnowing ty teulina,thot yh\n",
      "--------------------------\n",
      " tfacle.Tioe wole thau hist tn t\n",
      "--------------------------\n",
      " rg  aeve treverved Ayself to se\n",
      "--------------------------\n",
      " theesnsue \n",
      "\n",
      "PEULINA:\n",
      "Ihe e's nh\n",
      "--------------------------\n",
      "e tndugh nor hhet \n",
      "Aett them aea\n",
      "--------------------------\n",
      "ne tpon hhen srrh to haeuble Tou\n",
      "--------------------------\n",
      "etoy  oith tike aeeation.\n",
      "Wo tog\n",
      "--------------------------\n",
      "   r, Toursresious sotners arli \n",
      "--------------------------\n",
      " u  lnaltition Turtake th txery \n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Non auto-regressive sampling (works faster so you can see if it broadly making sense after 15 minutes)\n",
    "with jax.experimental.maps.mesh(*model.mesh_def):\n",
    "    y_pred = model.forward(state['params'], x)\n",
    "    y_pred_logit = jnp.argmax(y_pred, -1)\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(''.join([ds.itos[c] for c in list(y_pred_logit[i])]))\n",
    "        print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal example explaining the loss implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.3  0.  -0. ]\n",
      " [-0.   0.  -1.6]]\n",
      "3.719463\n",
      "[[ 0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.00945294]]\n",
      "1.6977012\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.8638048\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.5450409\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.39282578\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.30563444\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.24965373\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.21083269\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.18234465\n",
      "[[ 0. -0. -0.]\n",
      " [-0. -0.  0.]]\n",
      "0.16059524\n"
     ]
    }
   ],
   "source": [
    "# A test  explaining how the loss function works in simpler terms\n",
    "logit = jnp.array([[-0.3,1,0.1], [0.1, 2, 0.4]])\n",
    "target = jnp.array([[1,0,0], [0,0,1]])\n",
    "\n",
    "def loss(logit, target):\n",
    "    '''\n",
    "    An explainer loss function before we show the full one\n",
    "    '''\n",
    "    # numerically stabilise logits\n",
    "    stable_logit = logit - logit.max(-1, keepdims=True)\n",
    "    # zero out any entries in the logit which don't correspond to the true label\n",
    "    predicted_logit = jnp.multiply(target, stable_logit)\n",
    "    # sum up the logit\n",
    "    log_sum_exp = jnp.log(jnp.exp(stable_logit).sum(axis=-1))\n",
    "    # subtract the summed logit from the summed 'predicted_logit'\n",
    "    # Any entry but the correct one is 0 in 'predicted logit' - and due to the max used for stabilisation\n",
    "    # the entry of the highest index will be 0. Therefore, the subtraction of the two will draw the highest index to the correct one.\n",
    "    # By only working with sums when we are using the sharded version we minimise communication.\n",
    "    loss = log_sum_exp - predicted_logit.sum(axis=-1)\n",
    "    # And it allows for a really elegant way of calculating accuracy!\n",
    "    return loss.sum()\n",
    "    \n",
    "for i in range(0,10):\n",
    "    print(jnp.multiply(target, logit - logit.max(-1, keepdims=True)))\n",
    "    l, grad_wrt_logit  =  value_and_grad(loss)(logit, target)\n",
    "    logit -= 1 * grad_wrt_logit\n",
    "    print(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340184130\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_flatten, tree_unflatten\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tot = 0\n",
    "for j in state['params']:\n",
    "    for k in state['params'][j]:\n",
    "        shape = list(state['params'][j][k].shape)\n",
    "        s = 1\n",
    "        for f in shape:\n",
    "            s *= f\n",
    "        tot += s\n",
    "        \n",
    "print(tot)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
